{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\">Navigation</a>\n\n* [1. Libraries import](#1)\n* [2. Data loading](#2)\n* [3. Vizualization](#3)\n* [4. Features preprocessing](#4)\n    * [Age](#5)\n    * [Name](#6)\n    * [Ticket](#7)\n    * [Cabin](#8)\n    * [Fare](#9)\n    * [Sex](#10)\n    * [Embarked](#11)\n    * [SibSp, Parch](#12)\n* [5. Features choosing](#13)\n* [6. Normalization](#14)\n* [7. RandomizedSearch](#15)\n* [8. GridSearch](#16)\n* [9. Algorithms combine](#17)\n* [10. Final result voting](#18)\n* [11. Features importance](#19)"},{"metadata":{},"cell_type":"markdown","source":"# I want to write a few words about the goals and objectives of this notebook..\n\nof Course, the main goal is to achieve the maximum result in this competition, but not only.. I started my DS / ML career not so long ago and a lot of questions arise in the course of my work. In this notebook, I will try to give answers to the questions of beginners that I think they can ask, or that I myself would have asked at the very beginning..\nSo.. the result of this work will be submission. but let's understand in order what is needed for this\nI would conditionally split the whole process into 6 parts:\n1. Carefully read the terms of the contest!!!\nIt is important to understand what needs to be done, and most importantly how it will be evaluated!\n\n2. Upload the submitted data:\ncarefully study it, check for completeness and validity. You should understand the meaning of this data. If it is not clear, then look for an opportunity to solve this problem-search on the Internet, ask the organizers, ask your colleagues on kaggle. The point is that it is not possible to build a good model if you do not understand the logic of the process or the purpose of certain data.\n\n3. Processing and preparing data:\nYou need to study the data very carefully, select the fields that you intend to use in the model, and get rid of those that you don't think you need (you can always play this back).\nPut the data itself in order-delete or fill in the missing values, bring everything to the same view. All categorical data must be converted to a numeric form - for example, we have A, B, C-it must be 0,1,2 or 1,2,3, or male/female turns into 1/2.. and so on.\n\n4. Identifying features:\nthen the creative process begins:)\nnext, we will consider how you can get a valuable feature from information that would seem completely useless at first glance.\n\n5. there Is a set of features for the first run, you can start..\n\n6. Debugging and calibrating models - another one creative process:)) although it is more formalized and automated than the selection of features:)\n\nI also want to say right away that you will not find in this work a mega super cool model that gives 100500% accuracy.\nMaximum, I managed to get 0.80861, and it was just once. And so it gives an average of 0.795-0.805 if you play around with the settings."},{"metadata":{},"cell_type":"markdown","source":"# Хочу написать несоклько слов о целях и задачах этого ноутбука..\n\nКонечно, основная цель добиться максимального результате в этом соревновании, но не только.. я сам не так давно начал карьеру DS/ML и по ходу работы возникло много вопросов. В этом ноутбуке я попытаюсь дать ответы на вопросы новичков, которые, как мне кажется они могут задать, ну или которые я сам бы задал в самом начале..\nИтак.. итогом проведенной работы будет submission. но давайте по-порядку разбираться  , что для этого нужно\nя бы условно разбил весь процесс на 6 частей:\n1. Внимательно прочитать условия конкурса!!! \nВажно понять, что нужно сделать, и самое главное как это будет оцениваться!\n2. Загрузить представленные данные: \nВнимательно изучить их, проверить на полноту, валидность. Вам должен быть понятен смысл этих данных. Если не понятен, решайте эту проблему - ищите в интеренете, спрашивайте организаторов, спрашивайте у коллег на kaggle. Смысл в том, что не возможно построить хорошую модель, если вы не понимаете логику процесса или назначение тех или иных данных.\n3. Обработка и подготовка данных:\nНужно уже очень внимательно изучить данные, выбрать те поля, которые вы предполагаете использовать в модели, и избавиться от тех, которые, как вам кажется, не нужны (всегда можно будет это отыграть назад).\nПриведите сами данные в порядок  - удалите или заполните пропущенные значения, приведите все к одному виду. Все категориальные данные нужно привести к числовому виду - например имеем A, B, C - должно быть 0,1,2 или 1,2,3 или male/female превращается в 1/2.. ну и так далее.\n4. Выявление фичей:\nдальше начинется творчсекий процесс:)\nдальше будем рассматривать как из, казалось бы совершенно бесполезной на первый взгляд информации, можно получить ценную фичу\n5. Есть набор фичей для первого прогона, можно стартовать..\n6. Отладка и колибровка моделей - еще один творческий процесс:)) хотя и в большей степени формализован и автоматизирован, чем выбор фичей:)\n\nТак же сразу хочу сказать, что вы не найдете в этой работе мега супер крутой модели, которая выдает 100500 % accuracy.\nМаксимум, мне удалось получить 0.80861, и то 1 раз. А так выдает в среднем 79.5 - 80.5 если поиграться настройками."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\">Libraries import</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import colors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('numpy   ver: ', np.__version__)\nprint ('pandas  ver: ', pd.__version__)\nprint ('seaborn ver: ', sns.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, CategoricalNB, ComplementNB\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegressionCV, LarsCV, LassoCV, LassoLarsCV\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, RidgeClassifierCV\nfrom sklearn.linear_model import RidgeClassifier, ElasticNetCV, OrthogonalMatchingPursuit \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.semi_supervised import LabelPropagation\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import is_classifier, is_regressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\">Data loading</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data files into pandas DataFrames\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets merge dataframes before cleaning data\nfull = pd.merge(train, test, how = 'outer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\">Vizualization</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see on our data\nplt.figure(figsize=(16,7))\nsns.heatmap(full.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\">Age</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can use median to fill missed data on 'Age'\nfull.groupby(['Pclass', 'Sex'])['Age'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_null_age(cols):\n    Age,Pclass,Sex = cols\n    if pd.isnull(Age):\n        if Pclass == 1: \n            if Sex == 'female':\n                return 36\n            else:\n                return 42\n        elif Pclass == 2: \n            if Sex == 'female':\n                return 28\n            else:\n                return 29.5          \n        elif Pclass == 3: \n            if Sex == 'female':\n                return 22\n            else:\n                return 25  \n    else:\n        return Age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Age']=full[['Age', 'Pclass', 'Sex']].apply(set_null_age, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets round all ages\nfull.Age = full.Age.apply('ceil').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# im goint ot use ML to split ages for some clusters:)\n\nN = 6\nage = full.pivot_table(values = 'Survived', index = 'Age').sort_values('Age').reset_index()\nX = age[['Age', 'Survived']]\n\nclust = KMeans(n_clusters=N).fit(X)\nc = clust.cluster_centers_\n\n# we can take colors fro mathplotlib (10 colors - max 10 clusters)\nclrs = list(colors.TABLEAU_COLORS.keys())\n\nfig = plt.figure(figsize=(15, 3))\nfor x, y in zip(age.Age, age.Survived):\n    cl = clust.predict(np.array([x,y]).reshape(1, -1))\n    plt.scatter(x, y, s=75, c = clrs[cl[0]])\n    for i in range(len(c)):\n        plt.scatter(c[i][0], c[i][1], s=200, marker=\"x\", c=\"black\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another algorithm for clustering\n# N = 12\n# age = full.pivot_table(values = 'Survived', index = 'Age').sort_values('Age').reset_index()\n# X = age[['Age', 'Survived']]\n\n# clust = AgglomerativeClustering(n_clusters=N).fit(X)\n\n# lab =  clust.labels_\n# colors = ['g', 'y', 'm', 'r', 'c', 'b', 'g', 'y', 'm', 'r', 'c', 'b']\n\n# fig = plt.figure(figsize=(15, 3))\n# for x, y, z in zip(age.Age, age.Survived, lab):\n#     plt.scatter(x, y, s=75, c = colors[z])\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age['age_grouped'] = clust.labels_\nage = age.drop('Survived', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full = pd.merge(full, age, on = 'Age', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full[pd.isnull(full['age_grouped'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we fill set nearest clusters numbers for those rows\nfull.loc[972, 'age_grouped'] = full['age_grouped'][full.Age == 66].min()\nfull.loc[987, 'age_grouped'] = full['age_grouped'][full.Age == 74].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'age_grouped'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\">Name</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have some duplicates on 'Name'\nfull.Name.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full[full.duplicated('Name')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# its ok - they are different ppl\nfull[(full.Name == 'Kelly, Mr. James') | (full.Name == 'Connolly, Miss. Kate')].sort_values('Name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we dont really need names, but we can try to use Titles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Title'] = full['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Title'] = full['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'other')\n\nfull['Title'] = full['Title'].replace(['Mlle', 'Ms'], 'Miss')\nfull['Title'] = full['Title'].replace('Mme', 'Mrs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seems like a good feature\ncol = 'Title'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Title'] = full['Title'].map({\"Mr\": 1, \"other\": 2, \"Master\": 3, \"Miss\": 4, \"Mrs\": 5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\">Ticket</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# some tickets have diplicates in number.. we can try to use it\nfull['ticket_double'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['ticket_double'][full.duplicated('Ticket')] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# almost 50% survuve rate against 36% basic.. it can be the feature\ncol = 'ticket_double'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\">Cabin</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can try to extract cabin name and use it later as feature\nfull.Cabin.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Cabin'] = full['Cabin'].str.extract('([A-Za-z]+)', expand = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Cabin'] = full['Cabin'].map({'A':1, 'G':2, 'C':3, 'F':4, 'B':5, 'E':6, 'D':7, 'T':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Cabin'] = full['Cabin'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# also seems good for feature\ncol = 'Cabin'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\">Fare</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"full[pd.isnull(full.Fare) | (full.Fare == 0)].Fare.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full.groupby(['Pclass'])['Fare'].median().round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_null_fare(cols):\n    Fare, Pclass = cols\n    if Fare == 0 or pd.isnull(Fare):\n        if Pclass == 1:\n            return 63.36\n        elif Pclass == 2:\n            return 15.75\n        elif Pclass == 3:\n            return 8.05\n    else:\n        return Fare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Fare']=full[['Fare', 'Pclass']].apply(set_null_fare, axis = 1).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can try different classes for 'Fare'\ndef set_gr_fare(col):\n    Fare = col\n    if Fare < 8.05:\n        return 1\n    elif Fare < 15.75:\n        return 2\n    elif Fare < 32:\n        return 3\n    else:\n        return 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['fare_grouped']=full['Fare'].apply(set_gr_fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# also seems good for feature\ncol = 'fare_grouped'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\">Sex</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#very good feature\ncol = 'Sex'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full['Sex'] = full['Sex'].map({'male':1, 'female':2})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\">Embarked</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"full[pd.isnull(full.Embarked)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"..Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28..\"\n\nhttps://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"full.loc[(61,829), 'Embarked'] = 'S'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'Embarked'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will use 1, 2, 3 insteed of S, Q, C\nfull['Embarked'] = full['Embarked'].map({\"S\": 1, \"Q\": 2, \"C\": 3})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\">SibSp Parch</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can count family size\nfull['f_size'] = full.SibSp + full.Parch + 1\nfull['age_class'] = full.age_grouped * full.Pclass\nfull['is_alone'] = full['f_size'].apply(lambda x: 0 if x == 1 else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'f_size'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we dony anymore need those columns\nfull = full.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can use features as is.. but i want ot test dummies on all columns.. sometimes it gives better results. \n# But we can try both ways later\n\nbfull = full.drop(['Age', 'Fare'], axis = 1)\nbfull = bfull.fillna(0).astype(int)\n\ndummy_col=[ 'Pclass',\n            'Sex',\n            'Cabin',\n            'Embarked',\n            'age_grouped',\n            'Title',\n            'ticket_double',\n            'fare_grouped',\n            'f_size',\n            'is_alone']\ndummy = pd.get_dummies(bfull[dummy_col], columns=dummy_col)\nbfull = pd.concat([dummy, bfull], axis = 1)\nbfull.drop([\n            'Pclass',\n            'Sex',\n            'SibSp',\n            'Parch',\n            'Cabin',\n            'Embarked',\n            'age_grouped',\n            'Title',\n            'ticket_double',\n            'fare_grouped',\n            'f_size',\n            'age_class',\n            'is_alone'], inplace = True, axis = 1)\n\n\n# X_train = bfull[:891].drop('Survived', axis = 1).astype(int)\n# y_train = bfull[:891].Survived.astype(int)\n# X_test = bfull[891:].drop('Survived', axis = 1).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = full[:891].drop('Survived', axis = 1).astype(int)\n# y_train = full[:891].Survived.astype(int)\n# X_test = full[891:].drop('Survived', axis = 1).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\">Features choosing import</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of all features we have.. we will reduce that number later\nlist(bfull.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let see correlation rates\ndf = bfull[:891].astype(int)\ndf_all_corr = df.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"F1\", \"level_1\": \"feature\", 0: 'Corr'}, inplace=True)\nfinal_corr = df_all_corr[df_all_corr['F1'] == 'Survived']\nfinal_corr.Corr = final_corr.Corr.apply('abs').round(3)\nfinal_corr = final_corr.sort_values('Corr', ascending=False)\n\ncol = 'feature'\ntarget = 'Corr'\nsort = target\nprint(final_corr[1:].pivot_table(values = target, index = col).sort_values(sort, ascending=False))\nfinal_corr[1:].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'barh', figsize=(10,7), fontsize = 9)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets have dataset with features rates\nliverate = pd.DataFrame(list(bfull.columns)[:-1], columns = ['feature'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"liverate[[ 'nSurv', 'Surv', 'count']] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, feat in enumerate(liverate.feature):\n    col = feat\n    target = 'Survived'\n    sort = col\n    r = bfull[:891].pivot_table(values = target, index = col).sort_values(sort)\n    q = bfull[:891].groupby(feat).count()\n    liverate.loc[i, 'count'] = q.iloc[1,0]\n    liverate.loc[i, 'nSurv'] = r.loc[0,'Survived'].round(3)\n    liverate.loc[i, 'Surv'] = r.loc[1,'Survived'].round(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"liverate = pd.merge(liverate, final_corr[['feature', 'Corr']], on = 'feature', \n                    how = 'outer').sort_values('feature').reset_index().drop('index', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"liverate.sort_values('feature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have different ways to chose starting fetures from whole fetures list to test the difference \n# (there is almost no difference what features you start with, because we will reduce the number of them later)\ndf_nSurv = sorted(list(liverate.sort_values(['nSurv'], \n                                            ascending = False).head(20).feature) + ['Survived'])\ndf_Surv = sorted(list(liverate.sort_values(['Surv'], \n                                           ascending = False).head(20).feature) + ['Survived'])\ndf_count = sorted(list(liverate.sort_values(['count'], \n                                            ascending = False).head(20).feature) + ['Survived'])\ndf_corr = sorted(list(liverate.sort_values(['Corr'], ascending = False).head(21).feature))\n\ndf_mix = sorted(list(set(pd.concat([liverate.sort_values(['nSurv'], ascending = False).head(10).feature, \n                    liverate.sort_values(['Surv'], ascending = False).head(25).feature,\n                        liverate.sort_values(['count'], ascending = False).head(7).feature,                                                \n                        liverate.sort_values(['Corr'], ascending = False).head(8).feature]))))\ndf_max = list(liverate.feature)\ndf_mix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = df_mix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"60\">Next try array</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will put here best features after first models testing\n\ndf_next_try = ['Cabin_0',\n 'Embarked_3',\n 'Pclass_1',\n 'Pclass_2',\n 'Pclass_3',\n 'Sex_1',\n 'Sex_2',\n 'Title_1',\n 'Title_4',\n 'Title_5',\n 'age_grouped_2',\n 'age_grouped_4',\n 'f_size_1',\n 'f_size_3',\n 'is_alone_0'] + ['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = df_next_try","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bfull_test = bfull[start]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train0 = X_train = bfull_test[:891].drop('Survived', axis = 1).astype(int)\nX_test0 = bfull_test[891:].drop('Survived', axis = 1).astype(int)\ny_train0 = bfull_test[:891].Survived.astype(int)\n# y_test0 = pd.read_csv('test.csv').drop('PassengerId', axis  = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_train0, y_train0, test_size=0.25, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\">Normalization</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sometimes we need to make some preprocessing on our data\n# some algorithms show better results\n\ncolumns = list(X_train.columns)\n\nNZ = preprocessing.Normalizer()\nX_NZ = pd.DataFrame(NZ.fit_transform(X_train), columns = columns)\nRS = preprocessing.RobustScaler()\nX_RS = pd.DataFrame(RS.fit_transform(X_train), columns = columns)\nMM = preprocessing.MinMaxScaler()\nX_MM = pd.DataFrame(MM.fit_transform(X_train), columns = columns)\nSS = preprocessing.StandardScaler()\nX_SS = pd.DataFrame(SS.fit_transform(X_train), columns = columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plots = 5\nfig, ax = plt.subplots(ncols=plots, figsize=(20, 4))\n\ntitles  = ['Original', 'Normalizer', 'RobustScaler', 'MinMaxScaler', 'StandardScaler']\ndfs = [X_train, X_NZ, X_RS, X_MM, X_SS]\n\nfor i in range(plots):\n    ax[i].set_title(titles[i])\n    for col in columns:\n        sns.kdeplot(dfs[i][col], ax=ax[i], bw=0.15, legend = None)\n        ax[i].set_xlabel(None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\">RandomizedSearch</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can use 2 steps pretraing for models\n# 1st step - random test for some model parameters, its good, because RandomSearch works musch faster then GridSearch\n\nrfc = RandomForestClassifier()\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 800, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start = 3, stop = 15, num = 15)]\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 15)]\nmin_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 15)]\nbootstrap = [True, False]\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrs = RandomizedSearchCV(rfc, \n                        param_dist, \n                        n_iter = 200, \n                        cv = 3, \n                        verbose = 1, \n                        n_jobs=-1, \n                        random_state=0)\nrs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\nrs_df = rs_df.drop([\n            'mean_fit_time', \n            'std_fit_time', \n            'mean_score_time',\n            'std_score_time', \n            'params', \n            'split0_test_score', \n            'split1_test_score', \n            'split2_test_score', \n            'std_test_score'],\n            axis=1)\nrs_df.sort_values('mean_test_score', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['g', 'c', 'y',  'm', 'r', 'b']\ncolumns = list(rs_df.columns)\ny_min = round(rs_df['mean_test_score'].min(), 3)\ny_max = round(rs_df['mean_test_score'].max(), 3)\nfig, axs = plt.subplots(ncols=3, nrows=2, figsize=(25,15), )\nsns.set(style=\"whitegrid\", font_scale = 2)\n# fig.set_size_inches(25,18)\n\nfor i,k in enumerate(columns[:6]):\n    sns.barplot(x=k, y=columns[6], data=rs_df, color=colors[i], ax = axs[i//3,i%3])\n    axs[i//3,i%3].set_ylim((y_min, y_max))\n    axs[i//3,i%3].set_title(label = k[6:], weight='bold', fontsize = 20)\n    axs[i//3,i%3].set_xlabel(None,fontsize = 18)\n    axs[i//3,1].set_ylabel(None)\n    axs[i//3,2].set_ylabel(None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"16\">GridSearch</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we can use some of the nest params in GridSearch to tune the model\n\nrfc_2 = RandomForestClassifier()\nn_estimators = [722]\nmax_features = ['log2']\nmax_depth = [6, 7, 12, 15]\nmin_samples_split = [8, 32]\nmin_samples_leaf = [2,3,4,5,6,7,8]\nbootstrap = [False]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfc_2, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(X_train, y_train)\nrfc_3 = gs.best_estimator_\nbest_params = gs.best_params_\nbest_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rfc = RandomForestClassifier(**(best_params)).fit(X_train, y_train)\nY_pred_RF = model_rfc.predict(X_test)\nRF = round(metrics.accuracy_score(y_test, Y_pred_RF), 5)\nprint(RF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"17\">Algorithms combine</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# im going to start algorithm factory:) want it try all algorithms and compair it\n# but im too lazy to do it, so this ive automatised it:)\ns_names = ['RFC', 'LR', 'ENCV', 'OMP', 'LDA', 'QDA', 'RC', 'KNC', 'SVC', 'GNB', 'PCT', 'LSVC', 'SGDC', 'DTC', \n           'MLPC', 'LGBM', 'CBC', 'RCCV', 'ABC', 'PAC', 'LRCV', 'ETC', 'LLCV', 'ETsC', 'GBC', 'HGBC', 'NSVC', \n           'LPG', 'BC', 'CNB', 'LrCV', 'LsCV', 'XGBC'] \n\n\nalg = [RandomForestClassifier(), LogisticRegression(), ElasticNetCV(), OrthogonalMatchingPursuit(), \n       LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), RidgeClassifier(), KNeighborsClassifier(),\n       SVC(), GaussianNB(), Perceptron(), LinearSVC(), SGDClassifier(), DecisionTreeClassifier(), MLPClassifier(),\n       LGBMClassifier(), CatBoostClassifier(verbose=False), RidgeClassifierCV(), AdaBoostClassifier(),\n       PassiveAggressiveClassifier(), LogisticRegressionCV(), ExtraTreeClassifier(), LassoLarsCV(), \n       ExtraTreesClassifier(), GradientBoostingClassifier(), HistGradientBoostingClassifier(), NuSVC(), \n       LabelPropagation(), BaggingClassifier(), ComplementNB(), LarsCV(), LassoCV(), XGBClassifier()]\n\nalgs = pd.DataFrame(columns = ['name', 'algorithm', 's_name', \n                               'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2', 'fsc_1', 'fsc_2', 'matrix', 'nS_S', 'model'])\n\nalgs['algorithm'] = alg\nalgs['s_name'] = s_names\n\n\ndef add_column(col_name):\n    list_temp = []\n    for i in range(len(alg)):\n        list_temp.append({})\n    algs[col_name] = list_temp\n    \n    \n# we can use those params for Random or Grid testing in aotmat mode\nparam_grid =  {\n                'AdaBoostClassifier': {},\n                'BaggingClassifier': {},\n                'CatBoostClassifier': {},\n                'ComplementNB': {},\n                'DecisionTreeClassifier': {},\n                'ElasticNetCV': {},\n                'ExtraTreeClassifier': {},\n                'ExtraTreesClassifier': {},\n                'GaussianNB': {},\n                'GradientBoostingClassifier': {},\n                'HistGradientBoostingClassifier': {},\n                'KNeighborsClassifier': {},\n                'LGBMClassifier': \n                    {\n                        'iterations': [10, 20, 50, 100, 300, 500],\n                        'learning_rate': [0.01, 0.05, 0.1],\n                        'depth': [5, 7, 9, 11],\n                        'l2_leaf_reg': [1, 3, 5, 7, 9]\n                    },\n                'LabelPropagation': {},\n                'LarsCV': {},\n                'LassoCV': {},\n                'LassoLarsCV': {},\n                'LinearDiscriminantAnalysis': {},\n                'LinearSVC': {},\n                'LogisticRegression': {},\n                'LogisticRegressionCV': {},\n                'MLPClassifier': \n                    {\n                        'n_estimators': [25, 50, 100],\n                        'learning_rate': [0.01, 0.05, 0.1],\n                        'num_leaves': [7, 15, 31]\n                    },\n                'NuSVC': {},\n                'OrthogonalMatchingPursuit': {},\n                'PassiveAggressiveClassifier': {},\n                'Perceptron': {},\n                'QuadraticDiscriminantAnalysis': {},\n                'RandomForestClassifier': \n                    {\n                        'n_estimators': [20, 50, 200, 500, 800],\n                        'max_features': ['log2', 'sqrt'],\n                        'max_depth': [5, 7, 10],\n                        'min_samples_split': [1, 5, 10, 20],\n                        'min_samples_leaf': [2,3,4,5],\n                        'bootstrap': [False, True]\n                    },\n                'RidgeClassifier': {},\n                'RidgeClassifierCV': {},\n                'SGDClassifier': {},\n                'SVC': {},\n                'XGBClassifier': {}\n            }\n\n# we can use those params for algorithms in automode\nbest_params =  {\n                'AdaBoostClassifier': {},\n                'BaggingClassifier': {},\n                'CatBoostClassifier': {},\n                'ComplementNB': {},\n                'DecisionTreeClassifier': {},\n                'ElasticNetCV': {},\n                'ExtraTreeClassifier': {},\n                'ExtraTreesClassifier': {},\n                'GaussianNB': {},\n                'GradientBoostingClassifier': {},\n                'HistGradientBoostingClassifier': {},\n                'KNeighborsClassifier': \n                    {\n                        'n_neighbors': 4, \n                        'weights': 'uniform', \n                        'algorithm': 'brute'\n                    },\n                'LGBMClassifier': \n                    {\n                        'random_state': 0,\n                        'learning_rate': 0.01,\n                        'num_leaves': 3,\n                        'n_estimators': 100,\n                        'class_weight': 'balanced',\n                        'n_jobs': -1\n                    },\n                'LabelPropagation': {},\n                'LarsCV': {},\n                'LassoCV': {},\n                'LassoLarsCV': {},\n                'LinearDiscriminantAnalysis': {},\n                'LinearSVC': {},\n                'LogisticRegression': \n                    {\n                        'random_state': 1, \n                        'solver': 'liblinear', \n                        'penalty': 'l1'\n                    },\n                'LogisticRegressionCV': \n                    {'cv': 5,\n                        'random_state': 0,\n                        'dual': False,\n                        'penalty': 'l2',\n                        'solver': 'lbfgs',\n                        'max_iter': 100,\n                        'n_jobs': -1\n                    },\n                'MLPClassifier': \n                    {\n                        'solver': 'lbfgs',\n                        'alpha': 1e-05,\n                        'hidden_layer_sizes': (4, 3),\n                        'random_state': 0,\n                        'max_iter': 10000,\n                        'learning_rate': 'adaptive'\n                    },\n                'NuSVC': {},\n                'OrthogonalMatchingPursuit': {},\n                'PassiveAggressiveClassifier': {},\n                'Perceptron': \n                    {\n                        'random_state': 0, \n                        'penalty': 'l2', \n                        'max_iter': 5000\n                    },\n                'QuadraticDiscriminantAnalysis': {},\n                'RandomForestClassifier': \n                    {\n                        'n_estimators': 20,\n                        'criterion': 'gini',\n                        'max_features': 'log2',\n                        'max_depth': 8,\n                        'min_samples_split': 3,\n                        'min_samples_leaf': 13,\n                        'bootstrap': True,\n                        'random_state': 0\n                    },\n                'RidgeClassifier': {},\n                'RidgeClassifierCV': {},\n                'SGDClassifier': {},\n                'SVC': {},\n                'XGBClassifier': \n                    {\n                        'random_state': 0,\n                        'learning_rate': 0.01,\n                        'max_depth': 2,\n                        'n_estimators': 20,\n                        'n_jobs': -1\n                    }\n                }\n  \nnames = []\nfor i in alg:\n    names.append(str(i))\n\nalgs['names'] = names\n# algs['names'] = algs['algorithm'].apply('str')\nalgs['name'] = algs['names'].str.extract('([A-Za-z]+)', expand = False)\nalgs[algs.name == 'catboost']\nalgs.loc[16, 'name'] = 'CatBoostClassifier'\nalgs = algs.sort_values('name').reset_index().drop(['names', 'index'], axis  =1)\nalgs['param_grid'] = dict.fromkeys(algs['name'], {})\nadd_column('best_params')\nadd_column('best_model')\nalgs['type'] = algs['algorithm'].apply(lambda x: 'classifier'if is_classifier(x) else 'regressor'if is_regressor(x) else 'NA')\nalgs['param_grid'] = algs['name'].apply(lambda x: param_grid[x])\nalgs['best_params'] = algs['name'].apply(lambda x: best_params[x])\n# algs.to_csv ('algorithms_sklearn.csv', index = False, encoding = 'utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rfc_2 = RandomForestClassifier()\n# n_estimators = [720,800]\n# max_features = ['log2']\n# max_depth = [5, 7, 10]\n# min_samples_split = [8, 22]\n# min_samples_leaf = [2,3,4,5]\n# bootstrap = [False]\n# param_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n# gs = GridSearchCV(rfc_2, param_grid, cv = 3, verbose = 1, n_jobs=-1)\n# gs.fit(X_train, y_train)\n# rfc_3 = gs.best_estimator_\n# gs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Algorithm factory here. All target scores saved to algs DataFrame\nbest_score = 0\nbest_model = ''\nfor i, alg in enumerate(algs.algorithm):\n    print(alg)\n#     param_grid = algs.loc[i, 'param_grid']\n#     param_grid = {}\n    best_params  = algs.loc[i, 'best_params']\n    model = alg.set_params(**best_params)\n#     try:\n#         model = GridSearchCV(model, param_grid, cv = 5, scoring='accuracy', verbose = 0, n_jobs=-1).fit(X_train, y_train)\n#     except:\n#         continue\n    try:\n        model.fit(X_train, y_train)\n    except:\n        continue\n    Y_pred = model.predict(X_test).round()\n    accuracy_score = metrics.accuracy_score(y_test, Y_pred).round(5)\n    f1_score = metrics.f1_score(y_test, Y_pred, average=None).round(5)\n    pr_re_fscore  = metrics.precision_recall_fscore_support(y_test, Y_pred, average=None)\n    confusion_matrix = metrics.confusion_matrix(y_test, Y_pred)\n    if i == 27:\n        RFC_score = accuracy_score\n        RFC_model = model\n    algs.at[i, 'acc'] = accuracy_score\n    algs.at[i, 'pres_1'] = pr_re_fscore[0][0].round(5)\n    algs.at[i, 'pres_2'] = pr_re_fscore[0][1].round(5)\n    algs.at[i, 'rec_1'] = pr_re_fscore[1][0].round(5)\n    algs.at[i, 'rec_2'] = pr_re_fscore[1][1].round(5)\n    algs.at[i, 'fsc_1'] = pr_re_fscore[2][0].round(5)\n    algs.at[i, 'fsc_2'] = pr_re_fscore[2][1].round(5)\n    algs.at[i, 'matrix'] = confusion_matrix.round(5)\n    algs.at[i, 'nS_S'] = pr_re_fscore[3].round(5)\n    algs.at[i, 'model'] = model\n\n    \n#     algs.loc[i, 'best_params'] = model.best_params_\n#     algs.loc[i, 'best_model'] = model.best_estimator_\n# algs.to_csv ('class.csv', columns =['name', 'algorithm', 's_name', 'score'], index=False, encoding = 'utf-8')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\">Final result voting</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset with various csores from all algoriths finished\nalgs[['name', 's_name', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2', \n      'fsc_1', 'fsc_2', 'matrix', 'nS_S']].sort_values('acc', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now ill try to chose better combination of algorithm results to reach max accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# those give max acuracy (top4)\nt1 = algs[['name', 'algorithm', 'acc', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('acc', ascending = False)[0:4].reset_index().drop('index', axis  =1)\nt1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max pressision not Survived (top1)\nt2 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('pres_1', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max pressision Survived (top1)\nt3 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('pres_2', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max recall for not Suvived here (top1)\nt4 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('rec_1', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max recall for 'Suvived' here (top1)\nt5 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('rec_2', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#F-score for not Survived/Survived (top2 each)\ntf1  = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('fsc_1', ascending = False)[0:2].reset_index().drop('index', axis  =1)\ntf2  = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('fsc_2', ascending = False)[0:2].reset_index().drop('index', axis  =1)\n\ntf = pd.concat([tf1, tf2], axis=0).drop_duplicates('name')\ntf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#top TN/TP scores (top1)\ntm1 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2', 'matrix']][algs.type == 'classifier']\\\n                    .sort_values('matrix', key = lambda x: pd.Series(y[0][0] for y in x), \n                                 ascending = False)[0:1].reset_index().drop('index', axis  =1)\ntm2 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2', 'matrix']][algs.type == 'classifier']\\\n                    .sort_values('matrix', key = lambda x: pd.Series(y[0][1] for y in x), \n                                 ascending = False)[0:1].reset_index().drop('index', axis  =1)\n\ntm = pd.concat([tm1, tm2], axis=0).drop_duplicates('name')\ntm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = pd.concat([t1, t2, t3, t4, t5], axis=0).drop_duplicates('name')\nt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can play with different algorithms we want to use them at finishing Voting algorithm\nt_full=pd.concat([t, tf, tm], axis=0).drop_duplicates('name').reset_index().drop('index', axis=1)\nt_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t.drop([6], inplace=True)\n# t.drop([8], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So finishing Voting algorithm\nestimators  = list(zip(t_full.name, t_full.algorithm))\neclf1 = VotingClassifier(estimators=estimators, \n                     voting='hard').fit(X_train0, y_train0)\nY_pred_VT1 = eclf1.predict(X_test0).round()\n# accuracy_score = metrics.accuracy_score(y_test, Y_pred_VT1).round(5)\n# f1_score = metrics.f1_score(y_test, Y_pred_VT1, average=None).round(5)\n# pr_re_fscore  = metrics.precision_recall_fscore_support(y_test, Y_pred_VT1, average=None)\n# confusion_matrix = metrics.confusion_matrix(y_test, Y_pred_VT1)\n# print('acc   : ' , accuracy_score)\n# print('pres_1: ' , pr_re_fscore[0][0].round(5))\n# print('pres_2: ' , pr_re_fscore[0][1].round(5))\n# print('rec_1 : ' , pr_re_fscore[1][0].round(5))\n# print('rec_2 : ' , pr_re_fscore[1][1].round(5))\n# print('fsc_1 : ' , pr_re_fscore[2][0].round(5))\n# print('fsc_1 : ' , pr_re_fscore[2][1].round(5))\n# print('matrix: ' , confusion_matrix[0])\n# print('        ' , confusion_matrix[1])\n# metrics.plot_confusion_matrix(eclf1, X_test0, y_test0)\n# test0 = eclf1.predict(X_test0).round()\n# print(metrics.accuracy_score(y_test0, test0).round(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of cross-validation\ncross_validate(eclf1, X_test, y_test, return_train_score=True, return_estimator=True, cv=2, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"19\">Features importance</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = pd.DataFrame(RFC_model.feature_importances_.round(3), index=X_train0.columns, columns=['importance'])\nprint(imp.sort_values('importance', ascending = False).to_markdown())\nimp.sort_values('importance').plot(kind='barh', figsize=(10, 6), fontsize = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_feachers = sorted(list(imp.sort_values('importance', ascending = False)[0:15].index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets take best 15 features for next try\nbest_feachers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[set Next_try](#60)"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(columns = ['PassengerId', 'Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(Y_pred_VT1)):\n    submission.loc[i, 'PassengerId'] = i+892\n    submission.loc[i, 'Survived'] = int(round(Y_pred_VT1[i]))\nsubmission = submission.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[submission.Survived == 1].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv ('/kaggle/working/submission.csv', columns =['PassengerId', 'Survived'], index=False, encoding = 'utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can train different models separatelly to take a look how its work","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(random_state=1, solver='liblinear', penalty = 'l1').fit(X_train, y_train)\nY_pred_LR = model.predict(X_test).round()\nLR = round(metrics.accuracy_score(y_test, Y_pred_LR), 5)\nprint(LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ElasticNetCV(cv=5, random_state=0).fit(X_train, y_train)\nY_pred_EN = model.predict(X_test).round(0)\nEN = round(metrics.accuracy_score(y_test, Y_pred_EN), 5)\nprint(EN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = OrthogonalMatchingPursuit().fit(X_train, y_train)\nY_pred_OMP = model.predict(X_test).round(0)\nOMP = round(metrics.accuracy_score(y_test, Y_pred_OMP), 5)\nprint(OMP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearDiscriminantAnalysis().fit(X_train, y_train)\nY_pred_LDA = model.predict(X_test)\nLDA = round(metrics.accuracy_score(y_test, Y_pred_LDA), 5)\nprint(LDA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = QuadraticDiscriminantAnalysis().fit(X_train, y_train)\nY_pred_QDA = model.predict(X_test)\nQDA = round(metrics.accuracy_score(y_test, Y_pred_QDA), 5)\nprint(QDA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RidgeClassifier(random_state=0).fit(X_train, y_train)\nY_pred_RC = model.predict(X_test)\nRC = round(metrics.accuracy_score(y_test, Y_pred_RC), 5)\nprint(RC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors = 4, weights = 'uniform', algorithm = 'brute').fit(X_train, y_train)\nY_pred_KNN = model.predict(X_test)\nKNN = round(metrics.accuracy_score(y_test, Y_pred_KNN), 5)\nprint(KNN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GaussianNB().fit(X_train, y_train)\nY_pred_GNB = model.predict(X_test)\nGNB = round(metrics.accuracy_score(y_test, Y_pred_GNB), 5)\nprint(GNB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Perceptron(random_state=0, penalty = 'l2', max_iter = 5000).fit(X_train, y_train)\nY_pred_PCT = model.predict(X_test)\nPCT = round(metrics.accuracy_score(y_test, Y_pred_PCT), 5)\nprint(PCT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearSVC(random_state=0, max_iter = 10000).fit(X_train, y_train)\nY_pred_LSVC = model.predict(X_test)\nLSVC = round(metrics.accuracy_score(y_test, Y_pred_LSVC), 5)\nprint(LSVC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SGDClassifier().fit(X_train, y_train)\nY_pred_SGD = model.predict(X_test)\nSGD = round(metrics.accuracy_score(y_test, Y_pred_SGD), 5)\nprint(SGD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 1, min_samples_split = 2).fit(X_train, y_train)\nY_pred_DTC = model.predict(X_test)\nDTC = round(metrics.accuracy_score(y_test, Y_pred_DTC), 5)\nprint(DTC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(4, 3), \n                   random_state=0, max_iter = 10000, learning_rate = 'adaptive').fit(X_train, y_train)\nY_pred_MLPC = model.predict(X_test)\nMLPC = round(metrics.accuracy_score(y_test, Y_pred_MLPC), 5)\nprint(MLPC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBClassifier(random_state=0, learning_rate = 0.01, \n                              max_depth = 2, n_estimators = 20, n_jobs=-1).fit(X_train, y_train)\nY_pred_XGBC = model.predict(X_test)\nXGBC = round(metrics.accuracy_score(y_test, Y_pred_XGBC), 5)\nprint(XGBC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LGBMClassifier(random_state=0, learning_rate = 0.01, num_leaves = 3, n_estimators = 100, \n                                 class_weight='balanced').fit(X_train, y_train)\nY_pred_LGBM = model.predict(X_test)\nLGBM = round(metrics.accuracy_score(y_test, Y_pred_LGBM), 5)\nprint(LGBM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostClassifier(random_state=0, learning_rate = 0.01, l2_leaf_reg = 5,\n                                       depth = 5, iterations = 10, verbose=False).fit(X_train, y_train)\nY_pred_CBC = model.predict(X_test)\nCBC = round(metrics.accuracy_score(y_test, Y_pred_CBC), 5)\nprint(CBC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RidgeClassifierCV(cv=5).fit(X_train, y_train)\nY_pred_RCCV = model.predict(X_test)\nRCCV = round(metrics.accuracy_score(y_test, Y_pred_RCCV), 5)\nprint(RCCV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AdaBoostClassifier().fit(X_train, y_train)\nY_pred_ABC = model.predict(X_test)\nABC = round(metrics.accuracy_score(y_test, Y_pred_ABC), 5)\nprint(ABC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = PassiveAggressiveClassifier().fit(X_train, y_train)\nY_pred_PAC = model.predict(X_test)\nPAC = round(metrics.accuracy_score(y_test, Y_pred_PAC), 5)\nprint(PAC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegressionCV().fit(X_train, y_train)\nY_pred_LRCV = model.predict(X_test)\nLRCV = round(metrics.accuracy_score(y_test, Y_pred_LRCV), 5)\nprint(LRCV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExtraTreeClassifier().fit(X_train, y_train)\nY_pred_ETC = model.predict(X_test)\nETC = round(metrics.accuracy_score(y_test, Y_pred_ETC), 5)\nprint(ETC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegressionCV(cv=5, random_state=0, dual=False, \n                             penalty='l2', solver='lbfgs', max_iter=100, \n                             n_jobs=-1).fit(X_train, y_train)\nY_pred_LRCV = model.predict(X_test)\nLRCV = round(metrics.accuracy_score(y_test, Y_pred_LRCV), 5)\nprint(LRCV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LassoLarsCV().fit(X_train, y_train)\nY_pred_LLCV = model.predict(X_test).round()\nLLCV = round(metrics.accuracy_score(y_test, Y_pred_LLCV), 5)\nprint(LLCV)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = HistGradientBoostingClassifier().fit(X_train, y_train)\nY_pred_HGBC = model.predict(X_test)\nHGBC = round(metrics.accuracy_score(y_test, Y_pred_HGBC), 5)\nprint(HGBC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ExtraTreesClassifier().fit(X_train, y_train)\nY_pred_ETC = model.predict(X_test)\nETC = round(metrics.accuracy_score(y_test, Y_pred_ETC), 5)\nprint(ETC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = GradientBoostingClassifier().fit(X_train, y_train)\nY_pred_GBC = model.predict(X_test)\nGBC = round(metrics.accuracy_score(y_test, Y_pred_GBC), 5)\nprint(GBC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = NuSVC().fit(X_train, y_train)\nY_pred_NSVC = model.predict(X_test)\nNSVC = round(metrics.accuracy_score(y_test, Y_pred_NSVC), 5)\nprint(NSVC)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LabelPropagation().fit(X_train, y_train)\nY_pred_LP = model.predict(X_test)\nLP = round(metrics.accuracy_score(y_test, Y_pred_LP), 5)\nprint(LP)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Begin](#0)"},{"metadata":{},"cell_type":"markdown","source":"[Voting](#18)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}